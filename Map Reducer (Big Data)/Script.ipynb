{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import glob\n",
    "import time\n",
    "import operator\n",
    "import itertools\n",
    "import collections\n",
    "import string\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from scipy.spatial import distance\n",
    "import nltk\n",
    "from nltk import corpus\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup, SoupStrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook, you will learn about MapReduce, a popular paradigm for distributed computing on big data. Although you will not be working with actual \"big data\" (all the processing will happen on your computer with relatively small datasets), you will learn how to solve problems with the MapReduce programming model by writing your own map and reduce functions.\n",
    "\n",
    "### Grading\n",
    "- MR1: Inverted Index (11pts)\n",
    "- MR2: Reverse Web Graph (11pts)\n",
    "- MR3: K-Means Clustering (11pts)\n",
    "\n",
    "### Importing functions from unsupervised notebook\n",
    "You'll need your code from the unsupervised.ipynb notebook for the K-Means problem (MR3) in this notebook. Using the menu in Jupyter, you can export code from your unsupervised notebook as a Python script: \n",
    "1. Click File -> Download as -> Python (.py)\n",
    "2. Save file (unsupervised.py) in the same directory as this notebook \n",
    "3. (optional) Remove all test code (i.e. lines between AUTOLAB_IGNORE macros) from the script for faster loading time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Invalid GUI request 'svg', valid ones are:dict_keys(['inline', 'nbagg', 'notebook', 'ipympl', 'widget', None, 'qt4', 'qt', 'qt5', 'wx', 'tk', 'gtk', 'gtk3', 'osx', 'asyncio'])\n"
     ]
    }
   ],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "from unsupervised import KMeans\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce using Python Multiprocessing\n",
    "\n",
    "Autolab apparently doesn't communicate well with typical `mapreduce` libraries for Python available out there. So in this assignment, we have provided a very minimalistic framework of Map Reduce implemented using the `multiprocessing` library. Our code is built on top of this [PyMOTW](http://www.doughellmann.com/PyMOTW/) page: https://pymotw.com/3/multiprocessing/mapreduce.html. Please go through the code below and then the example (MR0) to get an idea of what goes under the hood of this framework and also how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2016 All rights reserved.\n",
    "\n",
    "# Redistribution and use in source and binary forms, with or without modification,\n",
    "# are permitted provided that the following conditions are met:\n",
    "\n",
    "# 1. Redistributions of source code must retain the above copyright notice, this\n",
    "# list of conditions and the following disclaimer.\n",
    "\n",
    "# 2. Redistributions in binary form must reproduce the above copyright notice,\n",
    "# this list of conditions and the following disclaimer in the documentation and/or\n",
    "# other materials provided with the distribution.\n",
    "\n",
    "# 3. Neither the name of the copyright holder nor the names of its contributors\n",
    "# may be used to endorse or promote products derived from this software without\n",
    "# specific prior written permission.\n",
    "\n",
    "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n",
    "# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n",
    "# WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
    "# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\n",
    "# ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n",
    "# (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n",
    "# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\n",
    "# ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
    "# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n",
    "# SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "\n",
    "def init_worker_context(context_to_set):\n",
    "    \"\"\"Called when each worker is initialized, it sets the context which will be shared across all\n",
    "    mappers.\n",
    "    \"\"\"\n",
    "    global context\n",
    "    # context can be accessed from the mapper function of every worker\n",
    "    context = context_to_set\n",
    "\n",
    "class MapReduceJob(object):\n",
    "    \n",
    "    def __init__(self, map_func, reduce_func, num_workers=None, worker_context=None):\n",
    "        \"\"\" Initialize a Map Reduce Job with the map, reduce functions, number of worker threads and the shared context\n",
    "        across mappers.\n",
    "        Args:\n",
    "            map_func:     function to map inputs to intermediate data. Takes as argument one input value and \n",
    "                            returns a tuple with the key and a value to be reduced.\n",
    "            reduce_func:  function to reduce partitioned version of intermediate data to final output. Takes as\n",
    "                            argument a key as produced by map_func and a list of the values associated with that key.\n",
    "            num_workers:  int, the number of workers to create in the pool. Defaults to #CPUs on the current host.\n",
    "            context:      any application-specific type, stores data that should be read-accessible from all workers\n",
    "                            but it should NOT be written while the MR job is in progress.\n",
    "        Attributes to set:\n",
    "            map_func:     function to map inputs to intermediate data. (same description as above)\n",
    "            reduce_func:  function to reduce intermediate data to final output. (same description as above)\n",
    "            pool:         multiprocessing.Pool object, with num_workers worker threads. The threads should initialize\n",
    "                            the worker context passed before running any map jobs.\n",
    "                            (See init_worker_context function above.)\n",
    "        \"\"\"\n",
    "        self.map_func = map_func\n",
    "        self.reduce_func = reduce_func\n",
    "        if worker_context is not None:\n",
    "            self.pool = multiprocessing.Pool(num_workers, initializer=init_worker_context, initargs=(worker_context,))\n",
    "        else:\n",
    "            self.pool = multiprocessing.Pool(num_workers)\n",
    "    \n",
    "    def partition(self, mapped_values):\n",
    "        \"\"\"Organize the mapped values by their key.\n",
    "        Args:\n",
    "            mapped_values: output key-value pairs from mappers\n",
    "        Outputs:\n",
    "            list:          returns an unsorted sequence of tuples with a key and a sequence of values.\n",
    "        \"\"\"\n",
    "        partitioned_data = collections.defaultdict(list)\n",
    "        for key, value in mapped_values:\n",
    "            partitioned_data[key].append(value)\n",
    "        return partitioned_data.items()\n",
    "\n",
    "    def __call__(self, inputs, chunksize=1):\n",
    "        \"\"\"Process the inputs through the map and reduce functions given.\n",
    "        Args:\n",
    "            inputs:       (array-like) contains the input data to be processed.\n",
    "            chunksize=1 : the portion of the input data to hand to each worker; can be used to tune performance \n",
    "                            during the mapping phase.\n",
    "        Outputs:\n",
    "            reduced_values: list of outputs from reduce functions.\n",
    "        \"\"\"\n",
    "        # partition inputs according to chunksize\n",
    "        indices = list(range(0, len(inputs), chunksize))\n",
    "#         print(indices, len(indices))\n",
    "        if indices[-1] != len(inputs):\n",
    "            indices.append(len(inputs))\n",
    "        inputs_split = [inputs[start:end] for start, end in zip(indices[:-1], indices[1:])]\n",
    "        # map\n",
    "        map_responses = self.pool.map(self.map_func, inputs_split)\n",
    "        # partition by key\n",
    "        partitioned_data = self.partition(itertools.chain(*map_responses))\n",
    "        # reduce\n",
    "        reduced_values = self.pool.map(self.reduce_func, partitioned_data)\n",
    "        return reduced_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick note: More details about `worker_context` is given in MR3. For MR0, MR1 and MR2, this should left as `None`.\n",
    "\n",
    "## MR0: Word Count (Example)\n",
    "A classic example of a MapReduce application is in the counting of words in a corpus. A corpus consists of several documents, which can be processed in parallel. The counts of words from each of these documents can be then be aggregated to produce the total count of each word in the corpus.\n",
    "\n",
    "### MapReduce Implementation Overview\n",
    "The map function processes each line of a file, emitting < word, 1> for each word encountered. The reduce function adds together all values for the same word and emits a < word, total count> pair.\n",
    "\n",
    "Now, go through the code below and make sure you understand how to use our implementation of the MapReduce framework given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mapreduce/wordcount/file1.rst', 'mapreduce/wordcount/file2.rst', 'mapreduce/wordcount/file3.rst', 'mapreduce/wordcount/file4.rst']\n",
      "Word-Count Map-Reduce job initialized.\n",
      "ForkPoolWorker-2 reading mapreduce/wordcount/file2.rst\n",
      "ForkPoolWorker-1 reading mapreduce/wordcount/file1.rst\n",
      "ForkPoolWorker-2 outputting [('2', 1), ('3', 1), ('4', 1)]\n",
      "ForkPoolWorker-1 outputting [('1', 1), ('2', 1), ('3', 1)]\n",
      "ForkPoolWorker-2 reading mapreduce/wordcount/file3.rst\n",
      "ForkPoolWorker-1 reading mapreduce/wordcount/file4.rst\n",
      "ForkPoolWorker-1 outputting [('6', 1), ('0', 1), ('1', 1)]\n",
      "ForkPoolWorker-2 outputting [('3', 1), ('4', 1), ('5', 1)]\n",
      "ForkPoolWorker-1 reducing ('1', [1, 1])\n",
      "ForkPoolWorker-2 reducing ('2', [1, 1])\n",
      "ForkPoolWorker-1 reducing ('4', [1, 1])\n",
      "ForkPoolWorker-2 reducing ('3', [1, 1, 1])\n",
      "ForkPoolWorker-2 reducing ('5', [1])\n",
      "ForkPoolWorker-1 reducing ('6', [1])\n",
      "ForkPoolWorker-2 reducing ('0', [1])\n",
      "Word-Count Map-Reduce job completed successfully.\n",
      "[('3', 3), ('4', 2), ('2', 2), ('1', 2), ('0', 1), ('6', 1), ('5', 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Volumes/Users/sngandhi/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Volumes/Users/sngandhi/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Volumes/Users/sngandhi/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Volumes/Users/sngandhi/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Volumes/Users/sngandhi/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Volumes/Users/sngandhi/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Volumes/Users/sngandhi/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Volumes/Users/sngandhi/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Volumes/Users/sngandhi/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Volumes/Users/sngandhi/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Volumes/Users/sngandhi/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Volumes/Users/sngandhi/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "def file_to_words(filenames):\n",
    "    \"\"\"Read a file and return a sequence of (word, occurances) values.\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    for filename in filenames:\n",
    "        print(multiprocessing.current_process().name, 'reading', filename)\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                for token in line.strip().split():\n",
    "                    output.append( (token, 1) )\n",
    "    print(multiprocessing.current_process().name, 'outputting', output)\n",
    "    return output\n",
    "\n",
    "def count_words(item):\n",
    "    \"\"\"Convert the partitioned data for a word to a\n",
    "    tuple containing the word and the number of occurances.\n",
    "    \"\"\"\n",
    "    print(multiprocessing.current_process().name, 'reducing', item)\n",
    "    word, occurances = item\n",
    "    return (word, sum(occurances))\n",
    "\n",
    "# AUTOLAB_IGNORE_START\n",
    "input_files = glob.glob('mapreduce/wordcount/*.rst')\n",
    "print(input_files)\n",
    "wordcount_job = MapReduceJob(file_to_words, count_words, num_workers=2)\n",
    "print('Word-Count Map-Reduce job initialized.')\n",
    "time.sleep(1)\n",
    "word_counts = wordcount_job(input_files, chunksize=1)\n",
    "print('Word-Count Map-Reduce job completed successfully.')\n",
    "word_counts.sort(key=operator.itemgetter(1))\n",
    "word_counts.reverse()\n",
    "print(word_counts)\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above code yields the following output:\n",
    "\n",
    "```\n",
    "['mapreduce/wordcount/file1.rst', 'mapreduce/wordcount/file2.rst', 'mapreduce/wordcount/file3.rst', 'mapreduce/wordcount/file4.rst']\n",
    "Word-Count Map-Reduce job initialized.\n",
    "ForkPoolWorker-3 reading mapreduce/wordcount/file2.rst\n",
    "ForkPoolWorker-4 reading mapreduce/wordcount/file1.rst\n",
    "ForkPoolWorker-4 outputting [('1', 1), ('2', 1), ('3', 1)]\n",
    "ForkPoolWorker-3 outputting [('2', 1), ('3', 1), ('4', 1)]\n",
    "ForkPoolWorker-4 reading mapreduce/wordcount/file3.rst\n",
    "ForkPoolWorker-3 reading mapreduce/wordcount/file4.rst\n",
    "ForkPoolWorker-4 outputting [('3', 1), ('4', 1), ('5', 1)]\n",
    "ForkPoolWorker-3 outputting [('6', 1), ('0', 1), ('1', 1)]\n",
    "ForkPoolWorker-3 reducing ('2', [1, 1])\n",
    "ForkPoolWorker-4 reducing ('1', [1, 1])\n",
    "ForkPoolWorker-4 reducing ('4', [1, 1])\n",
    "ForkPoolWorker-3 reducing ('3', [1, 1, 1])\n",
    "ForkPoolWorker-3 reducing ('6', [1])\n",
    "ForkPoolWorker-4 reducing ('5', [1])\n",
    "ForkPoolWorker-3 reducing ('0', [1])\n",
    "Word-Count Map-Reduce job completed successfully.\n",
    "[('3', 3), ('4', 2), ('2', 2), ('1', 2), ('0', 1), ('6', 1), ('5', 1)]\n",
    "```\n",
    "\n",
    "## MR1: Inverted Index (11pts)\n",
    "\n",
    "An inverted index is a data structure storing a mapping of elements to the list of locations at which the element occurs. In NLP, for example, given a corpora of documents, the elements are usually words, and locations are the document IDs (any unique identifier) and optionally, the position of the word in the document. Thus, a possible entry in the inverted index could be:\n",
    "\n",
    "*hello : (2, 1), (6, 4), (8,0), (8, 3)*\n",
    "\n",
    "and it is interpreted as \"the word *hello* is present in the 1st position in documents ID 2, 4th position in doc ID 6 and the 0th and 3rd positions in doc ID 8\".\n",
    "\n",
    "**Application:** The concept of an inverted index is central to any modern search engine. E.g., given a query \"X, Y\", where, we want to retrieve documents containing the words X or/and Y, the engine only needs to look at the union/intersection of the documents in the inverted indices of X and Y (which have been precomputed already)!\n",
    "\n",
    "### MapReduce Implementation Overview\n",
    "The map function parses each document, and emits a sequence of < word, (document ID, position) > pairs. The reduce function accepts all pairs for a given word, sorts the corresponding document IDs and emits a <word, list((document ID, position))> pair. The set of all output pairs forms a simple inverted index. It is easy to augment this computation to keep track of word positions.\n",
    "\n",
    "### Specifications\n",
    "- Words from a document titled `filename` in gutenberg corpus can be obtained using `nltk.corpus.gutenberg.words(filename)`.\n",
    "- For every word that is not a stop word, emit < word, (document ID, position) >. Use the stop list from NLTK's `nltk.corpus.stopwords.words('english')`. \n",
    "- The words themselves should NOT be processed in any way. E.g., no lemmatization/lower casing/etc.\n",
    "- Document ID is simply the name of the file, without any extension. E.g., document ID for `austen-emma.txt` is `austen-emma`.\n",
    "- The inverted index for a given word should be sorted first by the document ID and then by the position of thew ord within the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Volumes/Users/sngandhi/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Volumes/Users/sngandhi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Inverted-Index Map-Reduce job initialized.\n",
      "Inverted-Index Map-Reduce job completed successfully.\n",
      "! 5730\n",
      "!!!\" 1\n",
      "!\" 1719\n",
      "!\"' 3\n",
      "!\") 4\n",
      "!\"-- 47\n",
      "!\"?' 1\n",
      "!' 332\n",
      "!'\" 4\n",
      "!') 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Volumes/Users/sngandhi/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Volumes/Users/sngandhi/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Volumes/Users/sngandhi/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Volumes/Users/sngandhi/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Volumes/Users/sngandhi/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Volumes/Users/sngandhi/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Volumes/Users/sngandhi/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/Volumes/Users/sngandhi/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/Volumes/Users/sngandhi/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Volumes/Users/sngandhi/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Volumes/Users/sngandhi/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Volumes/Users/sngandhi/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Volumes/Users/sngandhi/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Volumes/Users/sngandhi/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Volumes/Users/sngandhi/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "  File \"/Volumes/Users/sngandhi/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Volumes/Users/sngandhi/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "def invindex_map(filenames):\n",
    "    \"\"\"Mapper: Read a list of filenames and generate (word, (document ID, position)) for every word in the filename.\n",
    "    Args:\n",
    "        filenames : list(str) : Filenames (from NLTK gutenberg corpus) to be processed.\n",
    "    Outputs:\n",
    "        (str, (str, int)) : tuple of word and location, which is in turn a tuple of document ID (str) and position of\n",
    "                                the word within document (int).\n",
    "    \"\"\"\n",
    "    \n",
    "    all_words = []\n",
    "    for filename in filenames:\n",
    "        file = nltk.corpus.gutenberg.words(filename)\n",
    "        name = filename.split('.')[0]\n",
    "        for i,word in enumerate(file):\n",
    "            if word not in stopwords.words('english'):\n",
    "                all_words.append((word,(name,i)))\n",
    "    return all_words\n",
    "\n",
    "def invindex_reduce(item):\n",
    "    \"\"\"Reducer: Given the list of locations of a word, concatenate them into a list \n",
    "    (sorted as per specifications).\n",
    "    Args:\n",
    "        item : (str, list((str, int))) :  first element is word\n",
    "    Outputs:\n",
    "        (str, list((str, int))) :   first element is the word; \n",
    "                            second element is the list of (document ID, position)\n",
    "    \"\"\"\n",
    "    word, name = item\n",
    "    return (word, sorted(name, key=lambda x: (x[0], x[1])))\n",
    "\n",
    "# AUTOLAB_IGNORE_START\n",
    "input_files = nltk.corpus.gutenberg.fileids()\n",
    "invindex_job = MapReduceJob(invindex_map, invindex_reduce, num_workers=4)\n",
    "print('Inverted-Index Map-Reduce job initialized.')\n",
    "time.sleep(1)\n",
    "invindex = invindex_job(input_files)\n",
    "print('Inverted-Index Map-Reduce job completed successfully.')\n",
    "invindex.sort(key=operator.itemgetter(0))\n",
    "for i, locations in invindex[:10]:\n",
    "    print(i, len(locations))\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation yields:\n",
    "```\n",
    "Inverted-Index Map-Reduce job initialized.\n",
    "Inverted-Index Map-Reduce job completed successfully.\n",
    "! 5730\n",
    "!!!\" 1\n",
    "!\" 1719\n",
    "!\"' 3\n",
    "!\") 4\n",
    "!\"-- 47\n",
    "!\"?' 1\n",
    "!' 332\n",
    "!'\" 4\n",
    "!') 1\n",
    "```\n",
    "\n",
    "## MR2: Reverse Web Graph (11pts)\n",
    "One way to represent a directed graph such as the web is using the directed adjacency list. The directed adjacency can be considered as the mapping from each node to the list of nodes it points to (i.e., out-neighbors). In the specific case of the web graph, this is fairly easy to construct because the out-neighbors (outgoing links) for a given webpage can be constricuted by parsing the webpage.\n",
    "\n",
    "However, suppose that we want the *reverse* adjacency list. That is, for each webpage, we want the list of webpages which point to it (i.e., incoming links). This is the reverse webgraph construction problem, for which an outline of the MapReduce solution is provided below.\n",
    "\n",
    "### MapReduce Implementation Overview\n",
    "The map function outputs < target ID, source ID > pairs for each link to a target URL found in a page named \"source\". The reduce function concatenates the list of all source URLs associated with a given target URL and emits the pair: < target ID, list(source IDs) >.\n",
    "\n",
    "### Specifications\n",
    "- Both target and sources pages should be HTML (i.e., .html files). Other links (e.g., to ps files) can be ignored.\n",
    "- A webpage (source/target) is identified by its complete URL. E.g.,  `http://cs.cornell.edu/Info/Courses/Current/CS415/CS414.html` is a valid webpage ID.\n",
    "- The URL of a webpage given its filename is obtained from the first line of the file.\n",
    "- Where only a partial (relative) URL is available (especially, when linked within href tag), create the complete URL using the hierarchical structure of source page's URL. For example, the file `page_0.html` contains a link to `cs415.html` in line 10. Hence, our code should output the following pair: \n",
    "\n",
    "`( http://cs.cornell.edu/Info/Courses/Current/CS415/cs415.html, http://cs.cornell.edu/Info/Courses/Current/CS415/CS414.html)`.\n",
    "\n",
    "- In the final output with the list of source webpage IDs for each target webpage ID, the source webpage IDs must be sorted alphabetically.\n",
    "- Finally, it should be noted a webpage which appears as a target may not appear as the source because the dataset is obtained from a limited crawl of the websites of these universities.\n",
    "\n",
    "Now, implement the map and reduce functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reverse-Webgraph Map-Reduce job initialized.\n",
      "Reverse-Webgraph Map-Reduce job completed successfully.\n",
      "http://www.cs.wisc.edu/~bestor/bestor.html 44\n",
      "http://www.gatech.edu/TechHome.html 39\n",
      "http://www.cc.gatech.edu/gvu/gvutop.html 36\n",
      "http://cs.nyu.edu/cs/courantnyu.html 33\n",
      "http://karna.cs.umd.edu:3264/people/minker.html 30\n",
      "http://www.cs.uiuc.edu/CS_INFO_SERVER/DEPT_INFO/CS_FACULTY/FAC_HTMLS/Faculty_Index.html 30\n",
      "http://www.cs.bu.edu/faculty/best/Home.html 26\n",
      "http://www.cs.wisc.edu/~cs302/cs302.html 26\n",
      "http://www.cs.wisc.edu/~cs302/Consultants/consultants.html 26\n",
      "http://www.ncsa.uiuc.edu/General/Internet/WWW/HTMLPrimer.html 25\n"
     ]
    }
   ],
   "source": [
    "def webgraph_map(filenames):\n",
    "    \"\"\"Mapper: Read a list of filenames (corresponding to HTML documents) and generate \n",
    "    (target ID, source ID) for every hyperlink in the file.\n",
    "    Args:\n",
    "        filenames : list(str) : Filenames to be processed.\n",
    "    Outputs:\n",
    "        (str, str) : tuple of target and source webpage IDs satisfying specifications.\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    for filename in filenames:\n",
    "        with open(filename, errors='ignore') as f:\n",
    "            lines = f.readlines()\n",
    "#             print(lines)\n",
    "        file_key = lines[0].strip()\n",
    "        html = \"\".join(lines[1:])\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            target = a['href']\n",
    "            if target.endswith('.html'):\n",
    "                target = a['href']\n",
    "                if '//' in a['href']:\n",
    "                    target = a['href']\n",
    "                else:\n",
    "                    target = '/'.join(file_key.split('/')[:-1])+'/'+a['href']\n",
    "                output.append((target, file_key))\n",
    "    return output\n",
    "\n",
    "def webgraph_reduce(item):\n",
    "    \"\"\"Reducer: Given the list of points belonging to a single cluster, compute the new means\n",
    "    Args:\n",
    "        item : (str, list(str)) :  first element is the webpage; \n",
    "                                    second element is the list of webpages linking to it\n",
    "    Outputs:\n",
    "        (str, list(str)) :   first element is the webpage; \n",
    "                                second element is the list of webpages linking to it, sorted\n",
    "                                in the specified order.\n",
    "    \"\"\"\n",
    "    target, sources = item\n",
    "    return (target, sorted(sources))\n",
    "\n",
    "# AUTOLAB_IGNORE_START\n",
    "input_files = glob.glob('mapreduce/webgraph/*.html')\n",
    "webgraph_job = MapReduceJob(webgraph_map, webgraph_reduce, num_workers=4)\n",
    "print('Reverse-Webgraph Map-Reduce job initialized.')\n",
    "time.sleep(1)\n",
    "webgraph = webgraph_job(input_files)\n",
    "print('Reverse-Webgraph Map-Reduce job completed successfully.')\n",
    "webgraph = sorted(webgraph, key=lambda x: -len(x[1]))\n",
    "for target, sources in webgraph[:10]:\n",
    "    print(target, len(sources))\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our code yields the following output:\n",
    "```\n",
    "Reverse-Webgraph Map-Reduce job initialized.\n",
    "Reverse-Webgraph Map-Reduce job completed successfully.\n",
    "http://www.cs.wisc.edu/~bestor/bestor.html 44\n",
    "http://www.gatech.edu/TechHome.html 39\n",
    "http://www.cc.gatech.edu/gvu/gvutop.html 36\n",
    "http://cs.nyu.edu/cs/courantnyu.html 33\n",
    "http://karna.cs.umd.edu:3264/people/minker.html 30\n",
    "http://www.cs.uiuc.edu/CS_INFO_SERVER/DEPT_INFO/CS_FACULTY/FAC_HTMLS/Faculty_Index.html 30\n",
    "http://www.cs.bu.edu/faculty/best/Home.html 26\n",
    "http://www.cs.wisc.edu/~cs302/cs302.html 26\n",
    "http://www.cs.wisc.edu/~cs302/Consultants/consultants.html 26\n",
    "http://www.ncsa.uiuc.edu/General/Internet/WWW/HTMLPrimer.html 25\n",
    "```\n",
    "\n",
    "## MR3: K-Means Clustering (11pts)\n",
    "In this section, you will implement the K-means algorithm from the unsupervised.ipynb notebook in the MapReduce framework. The outline of the algorithm is given below.\n",
    "\n",
    "### Algorithm\n",
    "```\n",
    "cluster_centers <- initialize clusters using Kmeans++ algorithm\n",
    "for i=1..T\n",
    "    cluster_centers <- output of the MapReduceJob which recomputes the new cluster_centers from the current cluster_centers\n",
    "end\n",
    "```\n",
    "\n",
    "### MapReduce Implementation Overview\n",
    "The map function outputs < cluster ID, point > pairs for each point based on the distance of the point from the current cluster centers. The reduce function collects the list of all points belonging to the same cluster and recomputes the cluster center: < cluster ID, recomputed cluster center >.\n",
    "A key difference from the previous MapReduce problems above is that the K-Means algorithm requires the information about the current cluster centers to be shared across all mappers. This is done by passing the `worker_context` variable when initializing the MapReduceJob.\n",
    "\n",
    "### Specifications\n",
    "- Use `scipy.spatial.distance.cdist(p1, p2)` to compute the distance between two points `p1` and `p2`.\n",
    "- When the distances of a point from two cluster centers are tied, assign the point to the cluster with lower ID.\n",
    "- The cluster center is recomputed by taking a mean of all points that belong to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'context' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-a974a179cd79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mapreduce/kmeans/faces_all.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mkmeans_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;31m# initial centers using KMeans++ algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# initial_centers = KMeans().init_centers(data,k)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-a974a179cd79>\u001b[0m in \u001b[0;36mkmeans_map\u001b[0;34m(points)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0mID\u001b[0m \u001b[0massigned\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpoint\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \"\"\"\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mcluster_centers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mcluster_assignments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'context' is not defined"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "from scipy import spatial\n",
    "from scipy.spatial import distance\n",
    "def kmeans_map(points):\n",
    "    \"\"\"Mapper: Read a list of points and assign each of them to a cluster.\n",
    "    Args:\n",
    "        points : (np.array) : N*d array of N d-dimensional points\n",
    "    Outputs:\n",
    "        (int, np.array) : tuple of the cluster ID assigned to the point and that point\n",
    "    \"\"\"\n",
    "    cluster_centers = context\n",
    "    cluster_assignments = np.argmin(scipy.distance.cdist(points, context), axis=1)\n",
    "    output =[]\n",
    "    for i, cluster in enumerate(cluster_assignment):\n",
    "        output.append((cluster, points[i]))\n",
    "    return output\n",
    "    \n",
    "def kmeans_reduce(item):\n",
    "    \"\"\"Reducer: Given the list of points belonging to a single cluster, compute the new means\n",
    "    Args:\n",
    "        item : (int, list(np.array)) :  first element is the cluster id (0,...,k-1); \n",
    "                                        second element is the list of d-dimensional points assigned to this cluster\n",
    "    Outputs:\n",
    "        (int, np.array) :   first element is the cluster id (0,...,k-1); \n",
    "                            second element is the recomputed cluster center, the mean of all points in this cluster\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def kmeans_train(data, initial_centers, num_iters, chunksize=1, num_workers=None):\n",
    "    \"\"\"Learns the Kmeans clustering of the given data starting from the given initialization of cluster centers\n",
    "    by creating `num_iters` sequential MapReduceJobs.\n",
    "    Inputs:\n",
    "        data: np.array : N*d array of points, where each row is a data point\n",
    "        initial_centers: np.array: k*d array of points, where each row is a center\n",
    "        chunksize: int: number of data points per mapper\n",
    "        num_iters: int: number of iterations (also, the number of MapReduceJobs you will create)\n",
    "        num_workers: int: number of workers in each MapReduceJobs\n",
    "    Outputs:\n",
    "        (array-like): N*1 array of cluster assignments\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# AUTOLAB_IGNORE_START\n",
    "# load data\n",
    "k = 25\n",
    "data = np.loadtxt(\"mapreduce/kmeans/faces_all.txt\")\n",
    "kmeans_map(data)\n",
    "# initial centers using KMeans++ algorithm\n",
    "# initial_centers = KMeans().init_centers(data,k)\n",
    "# # final cluster assignment using map reduce\n",
    "# print('K-means Map-Reduce job initialized.')\n",
    "# cluster_assignment = kmeans_train(data, initial_centers, 10, int(len(data)/20), 4)\n",
    "# print('K-means Map-Reduce job completed successfully.')\n",
    "# print(cluster_assignment)\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check your code by comparing with the previous homework. Do you get identical cluster assignments, for the same initialization of centers and iteration count?\n",
    "\n",
    "## For Fun\n",
    "Try playing with the `num_workers` values by varying it from 1 to 4 (typical number of cores in a machine, although you could have more/fewer!) and then beyond that. How does the running time change? Can you explain it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
